# Voice Interaction Architecture for AIOps Substrate

**Version:** 1.0
**Date:** 2025-11-07
**Purpose:** Add conversational voice interface for infrastructure management

---

## Overview

Transform the AIOps platform into a **voice-controlled infrastructure management system** where users can speak commands like:

> "Hey AIOps, add a DNS record for api.example.com pointing to 10.0.1.50"
>
> "Show me the status of all Kubernetes pods in the production namespace"
>
> "Scale up the web application to 5 replicas"

---

## Architecture

### Complete Voice Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User speaks into microphone                                     â”‚
â”‚  "Add DNS record for api.example.com to 10.0.1.50"             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: Voice Frontend (Web UI)                               â”‚
â”‚  â”œâ”€ Browser Web Audio API (capture audio)                       â”‚
â”‚  â”œâ”€ WebSocket connection to backend                             â”‚
â”‚  â”œâ”€ Real-time audio streaming                                   â”‚
â”‚  â””â”€ Audio playback for responses                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: Voice Gateway (FastAPI + WebSocket)                   â”‚
â”‚  â”œâ”€ /api/v1/voice/stream (WebSocket endpoint)                  â”‚
â”‚  â”œâ”€ Audio preprocessing (VAD, noise reduction)                  â”‚
â”‚  â”œâ”€ Session management                                          â”‚
â”‚  â””â”€ Audio format conversion                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: Speech-to-Text (Whisper)                              â”‚
â”‚  â”œâ”€ OpenAI Whisper (self-hosted)                               â”‚
â”‚  â”œâ”€ Model: whisper-base or whisper-small                       â”‚
â”‚  â”œâ”€ Transcribes audio â†’ text                                   â”‚
â”‚  â””â”€ Multiple language support                                   â”‚
â”‚                                                                  â”‚
â”‚  Input:  [audio bytes]                                          â”‚
â”‚  Output: "Add DNS record for api.example.com to 10.0.1.50"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 4: AI Ops Agent (Existing - FastAPI)                    â”‚
â”‚  â”œâ”€ Receives text from Whisper                                 â”‚
â”‚  â”œâ”€ Validates and enriches context                             â”‚
â”‚  â”œâ”€ Routes to Ollama for intent parsing                        â”‚
â”‚  â””â”€ Returns structured response                                 â”‚
â”‚                                                                  â”‚
â”‚  Input:  "Add DNS record..."                                    â”‚
â”‚  Output: {"action": "dns_add", "params": {...}}                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 5: LLM Processing (Ollama - Existing)                   â”‚
â”‚  â”œâ”€ Llama3.1:8b model                                          â”‚
â”‚  â”œâ”€ Parses natural language â†’ structured intent                â”‚
â”‚  â”œâ”€ Validates parameters                                        â”‚
â”‚  â””â”€ Returns execution plan                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 6: Infrastructure Execution (Existing)                   â”‚
â”‚  â”œâ”€ Ansible playbooks (DNS, services, etc.)                    â”‚
â”‚  â”œâ”€ kubectl commands (Kubernetes operations)                   â”‚
â”‚  â”œâ”€ Terraform (infrastructure changes)                          â”‚
â”‚  â””â”€ Proxmox API (VM management)                                â”‚
â”‚                                                                  â”‚
â”‚  Action: Execute "ansible-playbook dns-add.yml --extra-vars..." â”‚
â”‚  Result: {"status": "success", "message": "DNS record added"}  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 7: Text-to-Speech (Coqui TTS)                           â”‚
â”‚  â”œâ”€ Coqui TTS (self-hosted)                                    â”‚
â”‚  â”œâ”€ Model: tts_models/en/ljspeech/tacotron2-DDC               â”‚
â”‚  â”œâ”€ Converts response text â†’ speech                            â”‚
â”‚  â””â”€ Natural-sounding voice                                      â”‚
â”‚                                                                  â”‚
â”‚  Input:  "DNS record added successfully"                        â”‚
â”‚  Output: [audio bytes]                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 8: Voice Frontend (Playback)                            â”‚
â”‚  â”œâ”€ Receives audio via WebSocket                               â”‚
â”‚  â”œâ”€ Plays audio through browser                                â”‚
â”‚  â””â”€ Visual feedback (waveform, transcript)                      â”‚
â”‚                                                                  â”‚
â”‚  User hears: "DNS record for api.example.com added successfully"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Components to Add

### 1. Whisper STT Service (Speech-to-Text)

**Technology:** OpenAI Whisper (self-hosted)
**Model Size:** whisper-base (74M params, fast) or whisper-small (244M params, accurate)
**Languages:** 99+ languages supported
**Latency:** ~200-500ms for base model

**Docker Service:**
```yaml
whisper-stt:
  image: onerahmet/openai-whisper-asr-webservice:latest
  ports:
    - "9000:9000"
  environment:
    ASR_MODEL: base  # or small, medium, large
    ASR_ENGINE: openai_whisper
  volumes:
    - whisper-models:/root/.cache/whisper
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]  # Optional: GPU acceleration
```

**API Endpoint:**
- `POST /asr` - Transcribe audio file
- Input: Audio file (WAV, MP3, OGG, FLAC)
- Output: JSON with transcription + timestamps

### 2. Coqui TTS Service (Text-to-Speech)

**Technology:** Coqui TTS (Mozilla TTS successor)
**Model:** Tacotron2-DCC or VITS (fast, high quality)
**Voices:** Multiple pre-trained voices
**Latency:** ~100-300ms

**Docker Service:**
```yaml
coqui-tts:
  image: synesthesiam/coqui-tts:latest
  ports:
    - "5002:5002"
  volumes:
    - tts-models:/root/.local/share/tts
  command: >
    --model_name tts_models/en/ljspeech/tacotron2-DDC
    --vocoder_name vocoder_models/en/ljspeech/hifigan_v2
```

**API Endpoint:**
- `GET /api/tts?text=<text>` - Synthesize speech
- Input: Text string
- Output: Audio file (WAV)

### 3. Voice Gateway (New FastAPI Service)

**Purpose:** WebSocket gateway for real-time voice streaming

**Features:**
- WebSocket endpoint for bidirectional audio streaming
- Voice Activity Detection (VAD) - detect when user stops speaking
- Audio preprocessing (noise reduction, normalization)
- Session management (track conversation context)
- Rate limiting and authentication

**Endpoints:**
```python
# WebSocket for streaming audio
ws://localhost:8080/api/v1/voice/stream

# REST endpoints
POST /api/v1/voice/transcribe    # One-shot transcription
POST /api/v1/voice/synthesize     # One-shot TTS
GET  /api/v1/voice/status         # Check service health
```

### 4. Voice UI Frontend (React + Web Audio API)

**Technology Stack:**
- React 18 + TypeScript
- Web Audio API (capture microphone)
- WebSocket for real-time streaming
- Wavesurfer.js (audio visualization)
- Tailwind CSS (styling)

**Features:**
- Push-to-talk button (press Space to speak)
- Voice Activity Detection indicator
- Real-time transcription display
- Response playback with waveform
- Conversation history
- Dark mode support

---

## Implementation Plan

### Phase 1: Backend Services (Week 1)

#### Task 1.1: Add Services to Docker Compose
```yaml
# bootstrap/docker-compose.yml

services:
  # ... existing services (vault, ollama, minio, postgres)

  whisper-stt:
    image: onerahmet/openai-whisper-asr-webservice:latest
    container_name: aiops-whisper
    ports:
      - "9000:9000"
    environment:
      ASR_MODEL: base
      ASR_ENGINE: openai_whisper
    volumes:
      - whisper-models:/root/.cache/whisper
    networks:
      - aiops-network
    restart: unless-stopped

  coqui-tts:
    image: ghcr.io/coqui-ai/tts:latest
    container_name: aiops-tts
    ports:
      - "5002:5002"
    volumes:
      - tts-models:/root/.local/share/tts
    command: >
      tts-server
      --model_name tts_models/en/ljspeech/tacotron2-DDC
      --use_cuda false
    networks:
      - aiops-network
    restart: unless-stopped

  voice-gateway:
    build: ./services/voice-gateway
    container_name: aiops-voice-gateway
    ports:
      - "8080:8080"
    environment:
      WHISPER_URL: http://whisper-stt:9000
      TTS_URL: http://coqui-tts:5002
      AI_OPS_AGENT_URL: http://ai-ops-agent:30080
    depends_on:
      - whisper-stt
      - coqui-tts
    networks:
      - aiops-network
    restart: unless-stopped

volumes:
  whisper-models:
  tts-models:

networks:
  aiops-network:
    driver: bridge
```

#### Task 1.2: Create Voice Gateway Service

**Directory Structure:**
```
services/voice-gateway/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ websocket.py         # WebSocket handler
â”‚   â”œâ”€â”€ audio_processor.py   # Audio preprocessing
â”‚   â”œâ”€â”€ session_manager.py   # Conversation sessions
â”‚   â””â”€â”€ models.py            # Pydantic models
â””â”€â”€ tests/
    â””â”€â”€ test_voice_gateway.py
```

**main.py:**
```python
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import httpx
import asyncio

app = FastAPI(title="AIOps Voice Gateway", version="1.0.0")

# CORS for browser access
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.websocket("/api/v1/voice/stream")
async def voice_stream(websocket: WebSocket):
    """
    WebSocket endpoint for bidirectional voice streaming.

    Flow:
    1. Client sends audio chunks
    2. Gateway accumulates audio until VAD detects silence
    3. Send to Whisper for transcription
    4. Forward transcription to AI Ops Agent
    5. Get response and send to TTS
    6. Stream audio back to client
    """
    await websocket.accept()

    audio_buffer = bytearray()

    try:
        while True:
            # Receive audio chunk from client
            data = await websocket.receive_bytes()
            audio_buffer.extend(data)

            # Check if user stopped speaking (VAD)
            if is_silence_detected(audio_buffer):
                # Transcribe audio
                transcription = await transcribe_audio(audio_buffer)

                # Send transcription to client
                await websocket.send_json({
                    "type": "transcription",
                    "text": transcription
                })

                # Process with AI Ops Agent
                response = await process_command(transcription)

                # Synthesize speech
                audio_response = await synthesize_speech(response)

                # Send audio back to client
                await websocket.send_bytes(audio_response)

                # Clear buffer
                audio_buffer.clear()

    except WebSocketDisconnect:
        print("Client disconnected")

async def transcribe_audio(audio_bytes: bytes) -> str:
    """Send audio to Whisper STT service."""
    async with httpx.AsyncClient() as client:
        files = {"audio_file": audio_bytes}
        response = await client.post(
            "http://whisper-stt:9000/asr",
            files=files
        )
        return response.json()["text"]

async def process_command(text: str) -> str:
    """Send text to AI Ops Agent."""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://ai-ops-agent:30080/api/v1/intent",
            json={"nl": text}
        )
        return response.json()["message"]

async def synthesize_speech(text: str) -> bytes:
    """Convert text to speech using Coqui TTS."""
    async with httpx.AsyncClient() as client:
        response = await client.get(
            f"http://coqui-tts:5002/api/tts?text={text}"
        )
        return response.content

def is_silence_detected(audio_buffer: bytes) -> bool:
    """
    Detect silence using Voice Activity Detection.
    Returns True if user stopped speaking.
    """
    # Implement VAD logic (e.g., using Silero VAD)
    # Placeholder for now
    return len(audio_buffer) > 32000  # ~2 seconds at 16kHz
```

#### Task 1.3: Update Makefile

```makefile
# Voice interaction targets

voice-up: ## Start voice services (Whisper + TTS + Gateway)
	@echo "${GREEN}Starting voice services...${RESET}"
	$(COMPOSE) up -d whisper-stt coqui-tts voice-gateway
	@echo "Waiting for services to be ready..."
	@sleep 10
	@echo "${GREEN}Voice services ready!${RESET}"
	@echo "  Whisper STT: http://localhost:9000"
	@echo "  Coqui TTS:   http://localhost:5002"
	@echo "  Voice Gateway: ws://localhost:8080/api/v1/voice/stream"

voice-down: ## Stop voice services
	@echo "${GREEN}Stopping voice services...${RESET}"
	$(COMPOSE) rm -fsv whisper-stt coqui-tts voice-gateway

voice-test: ## Test voice pipeline
	@echo "${GREEN}Testing voice pipeline...${RESET}"
	# Test Whisper STT
	curl -X POST http://localhost:9000/asr \
		-F "audio_file=@test-audio.wav" | jq
	# Test Coqui TTS
	curl "http://localhost:5002/api/tts?text=Hello+AIOps" \
		--output test-output.wav
	@echo "${GREEN}âœ“ Voice services working!${RESET}"

voice-logs: ## Show voice service logs
	$(COMPOSE) logs -f whisper-stt coqui-tts voice-gateway
```

### Phase 2: Frontend (Week 2)

#### Task 2.1: Create Voice UI

**Directory Structure:**
```
frontend/voice-ui/
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ vite.config.ts
â”œâ”€â”€ index.html
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.tsx
â”‚   â”œâ”€â”€ main.tsx
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ VoiceButton.tsx
â”‚   â”‚   â”œâ”€â”€ Transcription.tsx
â”‚   â”‚   â”œâ”€â”€ Waveform.tsx
â”‚   â”‚   â””â”€â”€ ConversationHistory.tsx
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ useVoiceStream.ts
â”‚   â”‚   â”œâ”€â”€ useAudioCapture.ts
â”‚   â”‚   â””â”€â”€ useWebSocket.ts
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ audioProcessor.ts
â”‚       â””â”€â”€ vadDetector.ts
â””â”€â”€ public/
    â””â”€â”€ assets/
```

**App.tsx:**
```tsx
import React, { useState } from 'react';
import { VoiceButton } from './components/VoiceButton';
import { Transcription } from './components/Transcription';
import { ConversationHistory } from './components/ConversationHistory';
import { useVoiceStream } from './hooks/useVoiceStream';

export default function App() {
  const {
    isRecording,
    transcription,
    response,
    history,
    startRecording,
    stopRecording,
  } = useVoiceStream('ws://localhost:8080/api/v1/voice/stream');

  return (
    <div className="min-h-screen bg-gray-900 text-white p-8">
      <header className="text-center mb-12">
        <h1 className="text-4xl font-bold mb-2">
          ðŸŽ¤ AIOps Voice Assistant
        </h1>
        <p className="text-gray-400">
          Speak to manage your infrastructure
        </p>
      </header>

      <main className="max-w-4xl mx-auto space-y-8">
        {/* Voice Button */}
        <div className="flex justify-center">
          <VoiceButton
            isRecording={isRecording}
            onStart={startRecording}
            onStop={stopRecording}
          />
        </div>

        {/* Current Transcription */}
        {transcription && (
          <Transcription
            text={transcription}
            isProcessing={!response}
          />
        )}

        {/* Current Response */}
        {response && (
          <div className="bg-green-900/30 border border-green-500 rounded-lg p-4">
            <p className="text-lg">{response}</p>
          </div>
        )}

        {/* Conversation History */}
        <ConversationHistory items={history} />

        {/* Help */}
        <div className="bg-gray-800 rounded-lg p-6">
          <h3 className="text-xl font-semibold mb-4">
            Try saying:
          </h3>
          <ul className="space-y-2 text-gray-300">
            <li>"Add DNS record for api.example.com to 10.0.1.50"</li>
            <li>"Show me all pods in the production namespace"</li>
            <li>"Scale the web app to 5 replicas"</li>
            <li>"What's the status of the Vault service?"</li>
            <li>"Create a new virtual machine with 4 CPUs"</li>
          </ul>
        </div>
      </main>
    </div>
  );
}
```

**useVoiceStream.ts:**
```typescript
import { useState, useEffect, useRef } from 'react';

interface Message {
  type: 'user' | 'assistant';
  text: string;
  timestamp: Date;
}

export function useVoiceStream(wsUrl: string) {
  const [isRecording, setIsRecording] = useState(false);
  const [transcription, setTranscription] = useState('');
  const [response, setResponse] = useState('');
  const [history, setHistory] = useState<Message[]>([]);

  const wsRef = useRef<WebSocket | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);

  useEffect(() => {
    // Initialize WebSocket connection
    wsRef.current = new WebSocket(wsUrl);

    wsRef.current.onmessage = (event) => {
      const data = JSON.parse(event.data);

      if (data.type === 'transcription') {
        setTranscription(data.text);
        setHistory(prev => [...prev, {
          type: 'user',
          text: data.text,
          timestamp: new Date()
        }]);
      } else if (data.type === 'response') {
        setResponse(data.text);
        setHistory(prev => [...prev, {
          type: 'assistant',
          text: data.text,
          timestamp: new Date()
        }]);
        playAudio(data.audio);
      }
    };

    return () => {
      wsRef.current?.close();
    };
  }, [wsUrl]);

  const startRecording = async () => {
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        sampleRate: 16000
      }
    });

    audioContextRef.current = new AudioContext({ sampleRate: 16000 });
    const source = audioContextRef.current.createMediaStreamSource(stream);

    mediaRecorderRef.current = new MediaRecorder(stream);

    mediaRecorderRef.current.ondataavailable = (event) => {
      if (event.data.size > 0 && wsRef.current) {
        wsRef.current.send(event.data);
      }
    };

    mediaRecorderRef.current.start(100); // Send chunks every 100ms
    setIsRecording(true);
    setTranscription('');
    setResponse('');
  };

  const stopRecording = () => {
    mediaRecorderRef.current?.stop();
    mediaRecorderRef.current?.stream.getTracks().forEach(track => track.stop());
    setIsRecording(false);
  };

  const playAudio = (audioBase64: string) => {
    const audio = new Audio(`data:audio/wav;base64,${audioBase64}`);
    audio.play();
  };

  return {
    isRecording,
    transcription,
    response,
    history,
    startRecording,
    stopRecording
  };
}
```

### Phase 3: Testing (Week 3)

#### Task 3.1: Integration Tests

```python
# tests/integration/test_voice_pipeline.py

import pytest
import wave
import httpx
import asyncio

class TestVoicePipeline:
    """Test the complete voice pipeline."""

    @pytest.mark.integration
    @pytest.mark.voice
    async def test_whisper_stt(self):
        """Test Whisper Speech-to-Text."""
        # Load test audio file
        with open("test-audio.wav", "rb") as f:
            audio_data = f.read()

        async with httpx.AsyncClient() as client:
            files = {"audio_file": audio_data}
            response = await client.post(
                "http://localhost:9000/asr",
                files=files,
                timeout=30.0
            )

        assert response.status_code == 200
        result = response.json()
        assert "text" in result
        assert len(result["text"]) > 0
        print(f"Transcription: {result['text']}")

    @pytest.mark.integration
    @pytest.mark.voice
    async def test_coqui_tts(self):
        """Test Coqui Text-to-Speech."""
        text = "DNS record added successfully"

        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"http://localhost:5002/api/tts?text={text}",
                timeout=30.0
            )

        assert response.status_code == 200
        assert response.headers["content-type"] == "audio/wav"

        # Save audio file
        with open("test-output.wav", "wb") as f:
            f.write(response.content)

        # Verify it's a valid WAV file
        with wave.open("test-output.wav", "rb") as wav:
            assert wav.getnchannels() > 0
            assert wav.getframerate() > 0

    @pytest.mark.integration
    @pytest.mark.voice
    @pytest.mark.slow
    async def test_complete_voice_pipeline(self):
        """Test complete pipeline: audio â†’ transcription â†’ AI â†’ TTS â†’ audio."""
        # 1. Record/load test audio
        with open("test-command.wav", "rb") as f:
            audio_data = f.read()

        # 2. Transcribe
        async with httpx.AsyncClient() as client:
            stt_response = await client.post(
                "http://localhost:9000/asr",
                files={"audio_file": audio_data},
                timeout=30.0
            )
            transcription = stt_response.json()["text"]
            print(f"User said: {transcription}")

            # 3. Process with AI Ops Agent
            ai_response = await client.post(
                "http://localhost:30080/api/v1/intent",
                json={"nl": transcription},
                timeout=10.0
            )
            response_text = ai_response.json()["message"]
            print(f"AI responded: {response_text}")

            # 4. Synthesize speech
            tts_response = await client.get(
                f"http://localhost:5002/api/tts?text={response_text}",
                timeout=30.0
            )

            assert tts_response.status_code == 200

            # Save final audio
            with open("test-response.wav", "wb") as f:
                f.write(tts_response.content)

            print("âœ“ Complete voice pipeline test passed!")
```

### Phase 4: Documentation & Deployment

#### Task 4.1: Update Documentation

Create `docs/voice-interaction.md` with:
- Architecture overview
- Setup instructions
- API reference
- Troubleshooting guide
- Example commands

#### Task 4.2: Update Makefile targets

```makefile
help:
    # ... existing targets ...

    @echo "${GREEN}Voice Interaction:${RESET}"
    @echo "  voice-up         Start voice services"
    @echo "  voice-down       Stop voice services"
    @echo "  voice-test       Test voice pipeline"
    @echo "  voice-ui         Start voice UI (http://localhost:3000)"
    @echo "  voice-logs       Show voice service logs"
```

---

## Benefits of Voice Interaction

### 1. Accessibility
- âœ… Hands-free infrastructure management
- âœ… Multitasking while managing systems
- âœ… Accessibility for users with disabilities

### 2. Efficiency
- âœ… Faster than typing commands
- âœ… Natural language = no syntax memorization
- âœ… Immediate feedback

### 3. Innovation
- âœ… Conversational AI for DevOps
- âœ… Modern, cutting-edge interface
- âœ… Differentiator in the market

### 4. Use Cases
- ðŸŽ¤ "Scale up the production cluster"
- ðŸŽ¤ "What's the CPU usage on node 3?"
- ðŸŽ¤ "Deploy version 2.5 to staging"
- ðŸŽ¤ "Show me logs for the API pod"
- ðŸŽ¤ "Create a backup of the database"

---

## Security Considerations

### 1. Authentication
- Require API key or JWT token
- Voice biometric authentication (future)
- Session timeout (5 minutes idle)

### 2. Authorization
- Role-based access control (RBAC)
- Audit all voice commands
- Require confirmation for destructive actions

### 3. Privacy
- Optional: Don't store audio recordings
- Encrypt WebSocket connections (WSS)
- GDPR compliance (data retention policies)

---

## Performance Targets

| Component | Latency Target | Current |
|-----------|----------------|---------|
| **STT (Whisper)** | < 500ms | ~200-400ms (base model) |
| **AI Processing** | < 1s | ~500-800ms |
| **TTS (Coqui)** | < 300ms | ~100-250ms |
| **Total (speech â†’ speech)** | < 2s | ~1.5-2s |

**Optimizations:**
- Use GPU acceleration for Whisper (4x faster)
- Cache common TTS responses
- Streaming audio (don't wait for complete synthesis)
- VAD to reduce false triggers

---

## Cost Analysis

### Self-Hosted (Recommended)

| Component | CPU | RAM | Storage | Cost/Month |
|-----------|-----|-----|---------|------------|
| **Whisper (base)** | 2 cores | 2GB | 1GB models | Included in current setup |
| **Coqui TTS** | 1 core | 1GB | 500MB models | Included in current setup |
| **Voice Gateway** | 1 core | 512MB | - | Included in current setup |
| **Total Additional** | ~4 cores | ~3.5GB | ~1.5GB | **$0** (uses existing infra) |

### Cloud Alternative (Not Recommended - Vendor Lock-In)

| Service | Provider | Cost/1000 requests |
|---------|----------|-------------------|
| **STT** | Google Cloud Speech-to-Text | $1.44 |
| **TTS** | Google Cloud Text-to-Speech | $4.00 |
| **Total** | Per 1000 commands | **$5.44** |

**Self-hosted wins:** $0 vs $163/month (assuming 30,000 commands/month)

---

## Next Steps

1. âœ… **Review this architecture**
2. â­ï¸ **Add voice services to docker-compose.yml**
3. â­ï¸ **Create voice gateway service**
4. â­ï¸ **Build React voice UI frontend**
5. â­ï¸ **Write integration tests**
6. â­ï¸ **Update Makefile with voice targets**
7. â­ï¸ **Test complete pipeline**
8. â­ï¸ **Deploy and demo!**

---

**Estimated Timeline:** 2-3 weeks for full implementation

**Impact:** Transform from text-only to **conversational voice-controlled infrastructure platform** - truly next-generation DevOps!
